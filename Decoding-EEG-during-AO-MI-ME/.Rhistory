mi_sit_R_AO_train_no_high_leverage <- mi_sit_R_AO_train_no_high_leverage %>% filter(!(row_number %in% high_leverage_values.names)) %>% select(everything(), -row_number)
set.seed(42)
glm.fit <- glm(y ~ ., data = mi_sit_R_AO_subject_with_lowest_model_accuracy_training, family = "binomial")
glm.rstudent <- rstudent(glm.fit)
ggplot() +
geom_point(aes(
seq(1,length(glm.rstudent)), glm.rstudent,
), color = "#1A0875") +
geom_hline(yintercept = 3, color = '#a60808') +
geom_hline(yintercept = -3, color = '#a60808') +
labs(title = "Studentized Residuals Vs. Fitted Values", subtitle = "Detecting outliers in the model", x = "Index", y = "Studentized residuals") +
scale_y_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3)) +
scale_x_continuous(breaks = NULL)
f_print(sprintf("There are no detected outliers in the logistic regression fit on the subject with the lowest performing model's data."))
set.seed(42)
glm.fit <- glm(y ~ ., data = mi_sit_R_AO_subject_with_lowest_model_accuracy_training, family = "binomial")
glm.rstudent <- rstudent(glm.fit)
glm.rstudent(glm.fit)
set.seed(42)
glm.fit <- glm(y ~ ., data = mi_sit_R_AO_subject_with_lowest_model_accuracy_training, family = "binomial")
glm.rstudent <- rstudent(glm.fit)
glm.rstudent
ggplot() +
geom_point(aes(
seq(1,length(glm.rstudent)), glm.rstudent,
), color = "#1A0875") +
geom_hline(yintercept = 3, color = '#a60808') +
geom_hline(yintercept = -3, color = '#a60808') +
labs(title = "Studentized Residuals Vs. Fitted Values", subtitle = "Detecting outliers in the model", x = "Index", y = "Studentized residuals") +
scale_y_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3)) +
scale_x_continuous(breaks = NULL)
f_print(sprintf("There are no detected outliers in the logistic regression fit on the subject with the lowest performing model's data."))
c(glm.rstudent > 3 | glm.rstudent < -3)
glm.rs[c(glm.rstudent > 3 | glm.rstudent < -3)]
glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)]
glm.rstudent[c(glm.rstudent > 3 || glm.rstudent < -3)]
glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)]
near(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)], 0)
identical(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)], 0)
glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)]
length(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)])
near(length(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)]), 0)
set.seed(42)
glm.fit <- glm(y ~ ., data = mi_sit_R_AO_subject_with_lowest_model_accuracy_training, family = "binomial")
glm.rstudent <- rstudent(glm.fit)
ggplot() +
geom_point(aes(
seq(1,length(glm.rstudent)), glm.rstudent,
), color = "#1A0875") +
geom_hline(yintercept = 3, color = '#a60808') +
geom_hline(yintercept = -3, color = '#a60808') +
labs(title = "Studentized Residuals Vs. Fitted Values", subtitle = "Detecting outliers in the model", x = "Index", y = "Studentized residuals") +
scale_y_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3)) +
scale_x_continuous(breaks = NULL)
if((near(length(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)]), 0))){
f_print(sprintf("There are no detected outliers in the logistic regression fit on the subject with the lowest performing model's data."))
} else {
f_print(sprintf("There are %0.0f outliers detected in the data.", length(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)])))
print(glm.rstudent > 3 | glm.rstudent < -3)
}
print(glm.rstudent > 3 | glm.rstudent < -3)
glm.rstudent(print(glm.rstudent > 3 | glm.rstudent < -3))
glm.rstudent[print(glm.rstudent > 3 | glm.rstudent < -3)]
glm.rstudent[(glm.rstudent > 3 | glm.rstudent < -3)]
print(glm.rstudent[(glm.rstudent > 3 | glm.rstudent < -3)])
set.seed(42)
glm.fit <- glm(y ~ ., data = mi_sit_R_AO_subject_with_lowest_model_accuracy_training, family = "binomial")
glm.rstudent <- rstudent(glm.fit)
ggplot() +
geom_point(aes(
seq(1,length(glm.rstudent)), glm.rstudent,
), color = "#1A0875") +
geom_hline(yintercept = 3, color = '#a60808') +
geom_hline(yintercept = -3, color = '#a60808') +
labs(title = "Studentized Residuals Vs. Fitted Values", subtitle = "Detecting outliers in the model", x = "Index", y = "Studentized residuals") +
scale_y_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3)) +
scale_x_continuous(breaks = NULL)
if((near(length(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)]), 0))){
f_print(sprintf("There are no detected outliers in the logistic regression fit on the subject with the lowest performing model's data."))
} else {
f_print(sprintf("There are %0.0f outliers detected in the data.", length(glm.rstudent[c(glm.rstudent > 3 | glm.rstudent < -3)])))
print(glm.rstudent > 3 | glm.rstudent < -3)
}
set.seed(42)
# Identifying high-leverage point
p <- ncol(glm.fit$model)
n <- nrow(glm.fit$model)
# High-Leverage: value > 3 * (p number of parameters) / (n number of observations)
high_leverage_cutoff <- (3*p/n)
# Identifying high-leverage values
lm.hatvalues <- hatvalues(glm.fit)
high_leverage_values <- lm.hatvalues[lm.hatvalues > high_leverage_cutoff]
f_print(sprintf("There are %0.0f high-leverage values:", length(high_leverage_values)))
cat("\n")
high_leverage_values
high_leverage_values.names <- c(names(high_leverage_values))
high_leverage_values.names <- as.integer(high_leverage_values.names)
mi_sit_R_AO_train_no_high_leverage <- mi_sit_R_AO_train_list[[subject_with_lowest_model]] %>% mutate(row_number = row_number()) %>% select(row_number, everything())
mi_sit_R_AO_train_no_high_leverage <- mi_sit_R_AO_train_no_high_leverage %>% filter(!(row_number %in% high_leverage_values.names)) %>% select(everything(), -row_number)
set.seed(42)
glm.fit_no_high_leverage <- glm(y ~ ., data = mi_sit_R_AO_train_no_high_leverage, family = "binomial")
glm.prob_no_high_leverage <- predict(glm.fit_no_high_leverage, mi_sit_R_AO_subject_with_lowest_model_accuracy_validation, type = "response")
glm.pred_no_high_leverage <- rep(0, nrow(mi_sit_R_AO_train_list[[subject_with_lowest_model]][validation_set,]))
glm.pred_no_high_leverage[glm.prob_no_high_leverage > 0.5] <- 1
glm.table_no_high_leverage <- table(prediction = glm.pred_no_high_leverage, truth = mi_sit_R_AO_train_list[[subject_with_lowest_model]][validation_set,]$y)
glm.table_no_high_leverage
glm.accuracy_no_high_leverage <- (glm.table_no_high_leverage[1] + glm.table_no_high_leverage[4]) / sum(glm.table_no_high_leverage) * 100
f_print(sprintf("Validation Accuracy of Logistic Regression with no high leverage: %0.3f%%.", glm.accuracy_no_high_leverage))
set.seed(42)
glm.prob_test <- predict(glm.fit, mi_sit_R_AO_test_list[[2]], type = "response")
glm.pred_test <- rep(0, nrow(mi_sit_R_AO_test_list[[2]]))
glm.pred_test[glm.prob_test > 0.5] <- 1
# glm.accuracy_test_no_high_leverage
sum(glm.table_test)
glm.table_test <- table(prediction = glm.pred_test, truth = mi_sit_R_AO_test_list[[2]]$y)
glm.table_test
glm.accuracy_test_no_high_leverage <- (glm.table_test[1] + glm.table_test[4]) / sum(glm.table_test) * 100
f_print(sprintf("Accuracy of Logistic Regression on the subject with the lowest performing model's Test Data after removing high-leverage: %0.3f%%.", glm.accuracy_test_no_high_leverage))
# glm.accuracy_test_no_high_leverage
# sum(glm.table_test)
glm.table_test[1]
# glm.accuracy_test_no_high_leverage
# sum(glm.table_test)
glm.table_test[2]
# glm.accuracy_test_no_high_leverage
# sum(glm.table_test)
glm.table_test[3]
# glm.accuracy_test_no_high_leverage
# sum(glm.table_test)
glm.table_test[4]
# glm.accuracy_test_no_high_leverage
# sum(glm.table_test)
sum(glm.table_test)
glm.table_test <- table(prediction = glm.pred_test, truth = mi_sit_R_AO_test_list[[2]]$y)
glm.table_test
glm.accuracy_test_no_high_leverage <- (glm.table_test[1] + glm.table_test[4]) / sum(glm.table_test) * 100
f_print(sprintf("Accuracy of Logistic Regression on the subject with the lowest performing model's Test Data after removing high-leverage: %0.3f%%. The previous accuracy was: %0.3f.", glm.accuracy_test_no_high_leverage, subject_with_lowest_model.accuracy))
glm.table_test <- table(prediction = glm.pred_test, truth = mi_sit_R_AO_test_list[[2]]$y)
glm.table_test
glm.accuracy_test_no_high_leverage <- (glm.table_test[1] + glm.table_test[4]) / sum(glm.table_test) * 100
f_print(sprintf("Accuracy of Logistic Regression on the subject with the lowest performing model's Test Data after removing high-leverage: %0.3f%%. The previous accuracy on test data with an SVM was: %0.3f.", glm.accuracy_test_no_high_leverage, subject_with_lowest_model.accuracy))
glm.table_test <- table(prediction = glm.pred_test, truth = mi_sit_R_AO_test_list[[2]]$y)
glm.table_test
glm.accuracy_test_no_high_leverage <- (glm.table_test[1] + glm.table_test[4]) / sum(glm.table_test) * 100
f_print(sprintf("Accuracy of Logistic Regression on the subject with the lowest performing model's Test Data after removing high-leverage: %0.3f%%. The previous accuracy on test data with an SVM was: %0.3f%%.", glm.accuracy_test_no_high_leverage, subject_with_lowest_model.accuracy))
glm.table_no_high_leverage <- table(prediction = glm.pred_no_high_leverage, truth = mi_sit_R_AO_subject_with_lowest_model_accuracy_validation$y)
glm.table_no_high_leverage
glm.accuracy_no_high_leverage <- (glm.table_no_high_leverage[1] + glm.table_no_high_leverage[4]) / sum(glm.table_no_high_leverage) * 100
f_print(sprintf("Validation Accuracy of Logistic Regression with no high leverage: %0.3f%%.", glm.accuracy_no_high_leverage))
set.seed(42)
glm.prob_test <- predict(glm.fit, mi_sit_R_AO_subject_with_lowest_model_accuracy_test, type = "response")
set.seed(42)
# Creating a training and validation set for subject with the lowest performing model.
train_set <- sample(nrow(mi_sit_R_AO_train_list[[subject_with_lowest_model]]), size = nrow(mi_sit_R_AO_train_list[[subject_with_lowest_model]])*0.8)
validation_set <- c(integer(nrow(mi_sit_R_AO_train_list[[subject_with_lowest_model]]))*0.8)
index <- 1
for (i in seq(1, length(train_set))) {
if(!(i %in% train_set)) {
validation_set[[index]] <- i
index <- index + 1
}
}
mi_sit_R_AO_subject_with_lowest_model_accuracy_training <- mi_sit_R_AO_train_list[[subject_with_lowest_model]][train_set,]
mi_sit_R_AO_subject_with_lowest_model_accuracy_validation <- mi_sit_R_AO_train_list[[subject_with_lowest_model]][validation_set,]
mi_sit_R_AO_subject_with_loweset_model_accuracy_test <- mi_sit_R_AO_test_list[[subject_with_lowest_model]]
set.seed(42)
glm.prob_test <- predict(glm.fit, mi_sit_R_AO_subject_with_lowest_model_accuracy_test, type = "response")
set.seed(42)
# Creating a training and validation set for subject with the lowest performing model.
train_set <- sample(nrow(mi_sit_R_AO_train_list[[subject_with_lowest_model]]), size = nrow(mi_sit_R_AO_train_list[[subject_with_lowest_model]])*0.8)
validation_set <- c(integer(nrow(mi_sit_R_AO_train_list[[subject_with_lowest_model]]))*0.8)
index <- 1
for (i in seq(1, length(train_set))) {
if(!(i %in% train_set)) {
validation_set[[index]] <- i
index <- index + 1
}
}
mi_sit_R_AO_subject_with_lowest_model_accuracy_training <- mi_sit_R_AO_train_list[[subject_with_lowest_model]][train_set,]
mi_sit_R_AO_subject_with_lowest_model_accuracy_validation <- mi_sit_R_AO_train_list[[subject_with_lowest_model]][validation_set,]
mi_sit_R_AO_subject_with_lowest_model_accuracy_test <- mi_sit_R_AO_test_list[[subject_with_lowest_model]]
set.seed(42)
glm.prob_test <- predict(glm.fit, mi_sit_R_AO_subject_with_lowest_model_accuracy_test, type = "response")
glm.pred_test <- rep(0, nrow(mi_sit_R_AO_subject_with_lowest_model_accuracy_test))
glm.pred_test[glm.prob_test > 0.5] <- 1
glm.table_test <- table(prediction = glm.pred_test, truth = mi_sit_R_AO_subject_with_lowest_model_accuracy_test$y)
glm.table_test
glm.accuracy_test_no_high_leverage <- (glm.table_test[1] + glm.table_test[4]) / sum(glm.table_test) * 100
f_print(sprintf("Accuracy of Logistic Regression on the subject with the lowest performing model's Test Data after removing high-leverage: %0.3f%%. The previous accuracy on test data with an SVM was: %0.3f%%.", glm.accuracy_test_no_high_leverage, subject_with_lowest_model.accuracy))
glm.table_no_high_leverage <- table(prediction = glm.pred_no_high_leverage, truth = mi_sit_R_AO_subject_with_lowest_model_accuracy_validation$y)
glm.table_no_high_leverage
glm.accuracy_no_high_leverage <- (glm.table_no_high_leverage[1] + glm.table_no_high_leverage[4]) / sum(glm.table_no_high_leverage) * 100
f_print(sprintf("Validation Accuracy of Logistic Regression with no high leverage: %0.3f%%.", glm.accuracy_no_high_leverage))
set.seed(42)
glm.prob_test <- predict(glm.fit, mi_sit_R_AO_subject_with_lowest_model_accuracy_test, type = "response")
glm.pred_test <- rep(0, nrow(mi_sit_R_AO_subject_with_lowest_model_accuracy_test))
glm.pred_test[glm.prob_test > 0.5] <- 1
glm.table_test <- table(prediction = glm.pred_test, truth = mi_sit_R_AO_subject_with_lowest_model_accuracy_test$y)
glm.table_test
glm.accuracy_test_no_high_leverage <- (glm.table_test[1] + glm.table_test[4]) / sum(glm.table_test) * 100
f_print(sprintf("Accuracy of Logistic Regression on the subject with the lowest performing model's Test Data after removing high-leverage: %0.3f%%. The previous accuracy on test data with an SVM was: %0.3f%%.", glm.accuracy_test_no_high_leverage, subject_with_lowest_model.accuracy))
set.seed(42)
glm.fit_no_high_leverage <- glm(y ~ ., data = mi_sit_R_AO_train_no_high_leverage, family = "binomial")
glm.prob_no_high_leverage <- predict(glm.fit_no_high_leverage, mi_sit_R_AO_subject_with_lowest_model_accuracy_validation, type = "response")
glm.pred_no_high_leverage <- rep(0, nrow(mi_sit_R_AO_subject_with_lowest_model_accuracy_validation))
glm.pred_no_high_leverage[glm.prob_no_high_leverage > 0.5] <- 1
glm.table_no_high_leverage <- table(prediction = glm.pred_no_high_leverage, truth = mi_sit_R_AO_subject_with_lowest_model_accuracy_validation$y)
glm.table_no_high_leverage
glm.accuracy_no_high_leverage <- (glm.table_no_high_leverage[1] + glm.table_no_high_leverage[4]) / sum(glm.table_no_high_leverage) * 100
f_print(sprintf("Validation Accuracy of Logistic Regression with no high leverage: %0.3f%%.", glm.accuracy_no_high_leverage))
set.seed(42)
glm.prob_test <- predict(glm.fit, mi_sit_R_AO_subject_with_lowest_model_accuracy_test, type = "response")
glm.pred_test <- rep(0, nrow(mi_sit_R_AO_subject_with_lowest_model_accuracy_test))
glm.pred_test[glm.prob_test > 0.5] <- 1
glm.table_test <- table(prediction = glm.pred_test, truth = mi_sit_R_AO_subject_with_lowest_model_accuracy_test$y)
glm.table_test
glm.accuracy_test_no_high_leverage <- (glm.table_test[1] + glm.table_test[4]) / sum(glm.table_test) * 100
f_print(sprintf("Accuracy of Logistic Regression on the subject with the lowest performing model's Test Data after removing high-leverage: %0.3f%%. The previous accuracy on test data with an SVM was: %0.3f%%.", glm.accuracy_test_no_high_leverage, subject_with_lowest_model.accuracy))
set.seed(42)
subject_2_svm_no_high_leverage <- tune(svm, y ~ ., data = mi_sit_R_AO_train_no_high_leverage, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 25, 50, 100, 1000)))
set.seed(42)
subject_2_svm_no_high_leverage.pred <- predict(subject_2_svm_no_high_leverage$best.model, mi_sit_R_AO_subject_with_lowest_model_accuracy_test)
subject_2_svm_no_high_leverage.table <- table(prediction = subject_2_svm_no_high_leverage.pred, truth = mi_sit_R_AO_subject_with_lowest_model_accuracy_test$y)
subject_2_svm_no_high_leverage.table
subject_2_svm_no_high_leverage.accuracy <- (table[1] + table[4]) / sum(table) * 100
subject_2_svm_no_high_leverage.accuracy
f_print(sprintf("The validation accuracy of the logistic regression model on the subject with the lowest performing model's data increased model performance from the test-set accuracy of %0.3f%% to %0.3f%% after removing high-leverage values detected in the subject with the lowest performing model's training data. After refitting the svm without the high leverage observations, the accuracy of the svm predictions driven by the test set increased by %0.3f%% to %0.3f%%.", subject2_model_accuracy[[2]], glm.accuracy_no_high_leverage, subject_2_svm_no_high_leverage.accuracy - subject2_model_accuracy[[2]], subject_2_svm_no_high_leverage.accuracy))
f_print(sprintf("The validation accuracy of the logistic regression model on the subject with the lowest performing model's data increased model performance from the test-set accuracy of %0.3f%% to %0.3f%% after removing high-leverage values detected in the subject with the lowest performing model's training data. After refitting the svm without the high leverage observations, the accuracy of the svm predictions driven by the test set increased by %0.3f%% to %0.3f%%.", subject2_model_accuracy[[subject_with_lowest_model]], glm.accuracy_no_high_leverage, subject_2_svm_no_high_leverage.accuracy - subject2_model_accuracy[[subject_with_lowest_model]], subject_2_svm_no_high_leverage.accuracy))
f_print(sprintf("The validation accuracy of the logistic regression model on the subject with the lowest performing model's data increased model performance from the test-set accuracy of %0.3f%% to %0.3f%% after removing high-leverage values detected in the subject with the lowest performing model's training data. After refitting the svm without the high leverage observations, the accuracy of the svm predictions driven by the test set increased by %0.3f%% to %0.3f%%.", subject_with_lowest_model, glm.accuracy_no_high_leverage, subject_2_svm_no_high_leverage.accuracy - subject_with_lowest_model, subject_2_svm_no_high_leverage.accuracy))
f_print(sprintf("The validation accuracy of the logistic regression model on the subject with the lowest performing model's data increased model performance from the test-set accuracy of %0.3f%% to %0.3f%% after removing high-leverage values detected in the subject with the lowest performing model's training data. After refitting the svm without the high leverage observations, the accuracy of the svm predictions driven by the test set increased by %0.3f%% to %0.3f%%.", subject_with_lowest_model.accuracy, glm.accuracy_no_high_leverage, subject_2_svm_no_high_leverage.accuracy - subject_with_lowest_model.accuracy, subject_2_svm_no_high_leverage.accuracy))
mi_sit_R_AO_x_train <- mi_sit_R_AO_train_list[[subject_with_lowest_model]] %>% select(everything(), -y)
mi_sit_R_AO_y_train <- mi_sit_R_AO_train_list[[subject_with_lowest_model]]$y
reticulate::repl_python()
mi_sit_R_AO_x_train_nn <- cbind(py$mi_sit_R_AO_x_train_formatted$fb1, py$mi_sit_R_AO_x_train_formatted$fb2, py$mi_sit_R_AO_x_train_formatted$fb3, py$mi_sit_R_AO_x_train_formatted$fb4, py$mi_sit_R_AO_x_train_formatted$fb5, py$mi_sit_R_AO_x_train_formatted$fb6, py$mi_sit_R_AO_x_train_formatted$fb7, py$mi_sit_R_AO_x_train_formatted$fb8, py$mi_sit_R_AO_x_train_formatted$fb9)
mi_sit_R_AO_y_train_nn <- cbind(as.double(py$mi_sit_R_AO_y_train_formatted))
dim(mi_sit_R_AO_x_train_nn)
modellr <- keras_model_sequential() %>%
layer_dense(input_shape = c(9), units = 10) %>%
layer_dense(units = 1, activation = "sigmoid")
modellr %>% compile(loss = "mse", optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
modellr %>% fit(mi_sit_R_AO_x_train_nn, mi_sit_R_AO_y_train_nn, epochs = 30, batch_size = , validation_split = 0.2, verbose = 0)
dim(mi_sit_R_AO_x_train_nn)
f_print(sprintf("The validation accuracy of the logistic regression model on the subject with the lowest performing model's data increased model performance from %0.3f%% to %0.3f%% after removing high-leverage values detected in the subject with the lowest performing model's training data. After refitting the svm without the high leverage observations, the accuracy of the svm predictions driven by the test set increased by %0.3f%% to %0.3f%%.", glm.accuracy, glm.accuracy_no_high_leverage, subject_2_svm_no_high_leverage.accuracy - subject_with_lowest_model.accuracy, subject_2_svm_no_high_leverage.accuracy))
f_print(sprintf("The validation accuracy of the logistic regression model on the subject with the lowest performing model's data increased model performance from %0.3f%% to %0.3f%% after removing high-leverage values detected in the subject with the lowest performing model's training data. After refitting the svm without the high leverage observations, the accuracy of the svm predictions driven by the test set increased by %0.3f%% to %0.3f%%.", glm.accuracy, glm.accuracy_no_high_leverage, subject_2_svm_no_high_leverage.accuracy - subject_with_lowest_model.accuracy, subject_2_svm_no_high_leverage.accuracy))
set.seed(42)
subject_2_svm_no_high_leverage <- tune(svm, y ~ ., data = mi_sit_R_AO_train_no_high_leverage, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 25, 50, 100, 1000)))
f_print(sprintf("The validation accuracy of the logistic regression model on the subject with the lowest performing model's data increased model performance from %0.3f%% to %0.3f%% after removing high-leverage values detected in the subject with the lowest performing model's training data. After refitting the support vector classifier without the high leverage observations, the accuracy of the support vector classifier's predictions driven by the test set changed by %0.3f%% from %0.3f%% to %0.3f%%.", glm.accuracy, glm.accuracy_no_high_leverage, subject_2_svm_no_high_leverage.accuracy - subject_with_lowest_model.accuracy, subject_with_lowest_model.accuracy, subject_2_svm_no_high_leverage.accuracy))
notch = {'f0': 50}
reticulate::repl_python()
apply_eeg_preprocessing(subject_name=subjects[i], session='me', task='sit', filter_method = filter_methods)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
comment = ""
)
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning
if(!require("jpeg")) install.packages("jpeg")
if(!require("imager")) install.packages("imager")
if(!require("tensorflow")) install.packages("te")
library(tensorflow)
library(imager)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
library(jpeg)
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
f_print <- function(string){
cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
source_scripts <- function(){
base_dir <- 'pysitstand/'
eeg_py <- str_c(base_dir, 'eeg.py')
eeg_preprocessing_py <- str_c(base_dir, 'eeg_preprocessing.py')
emg_preprocessing_py <- str_c(base_dir, 'emg_preprocessing.py')
info_py <- str_c(base_dir, 'info.py')
model_py <- str_c(base_dir, 'model.py')
utils_py <- str_c(base_dir, 'utils.py')
source_python(eeg_py)
source_python(eeg_preprocessing_py)
source_python(emg_preprocessing_py)
source_python(info_py)
source_python(model_py)
source_python(utils_py)
}
reticulate::repl_python()
# """ Create Train and Test Sets
#
#   Purpose
#   -------
#   This function will split the data into training and test sets such that one
#   trial of 22 observations is reserved for testing. There are 15 trials in
#   total. This data is expected to have been pre-processed with a butterworth
#   bandpass filter, downsampled to 250 Hz, and free from artifacts via
#   independent component analysis. This data will next need to be processed via
#   a filter bank common spatial pattern before being modeled.
#
#   Parameters
#   ----------
#   x: A numpy array of pre-processed signal test and train data for binary
#     classification. The data is to be split into train and test, and contains
#     either resting vs AO or AO vs MI for both sit-to-stand and stand-to-sit
#     situations for motor imagery. This data has not yet been through the final
#     processing step involving a filter bank common spatial pattern.
#
#   y: A numpy array of pre-processed truth values test & train data for binary
#     classification. The data is to be split into train and test, and contains
#     either resting vs AO or AO vs MI for both sit-to-stand and stand-to-sit
#     situations for motor imagery. This data has not yet been through the final
#     processing step involving a filter bank common spatial pattern.
#
#    Returns
#    -------
#    X_train: A np array of pre-processed EEG signals. Consists of 14 trials.
#             The shape is trials by channels by time points where time points
#             is the window size of 2 seconds multiplied by the sample frequency
#             of 250 Hz. Each trial is 22 observations in the first dimension.
#             This data is anticipated to be used for training and validation.
#
#    y_train: A np array of truth values pertaining to class by phase. Values
#             are integers including 0 and 1.
#
#    X_test: A np array of pre-processed EEG signals. Consists of 14 trials.
#             The shape is trials by channels by time points where time points
#             is the window size of 2 seconds multiplied by the sample frequency
#             of 250 Hz. Each trial is 22 observations in the first dimension.
#             This data is anticipated to be used for testing.
#
#    y_test: A np array of truth values pertaining to class by phase. Values
#             are integers including 0 and 1.
# """
set.seed(42)
create_train_and_test_sets <- function(x, y){
py$train_index <- sample(nrow(x), size = 308)
py$test_index <- c(integer(nrow(y) * (1/15)))
index <- 1
for (i in seq(1, 330)){
if (!(i %in% py$train_index)){
py$test_index[index] <- i
index <- index + 1
}
}
py$X_train = x[py$train_index,,]
py$y_train = y[py$train_index]
py$X_test = x[py$test_index,,]
py$y_test = y[py$test_index]
}
reticulate::repl_python()
# """Fit a linear SVC Model
# Purpose
# -------
# Fit a SVC models for four classes during the Motor imagery session. The phases are Resting vs. Action Observation for sit-to-stand and stand-to-sit situations,
#  and Action Observation vs. Motor Imagery for sit-to-stand and stand-to-sit situations.
#
# Parameters
# ----------
# model_list: List used to store each of the 8 models created.
# train_list: List of training data. Each item in the list represents a subject and the data represents a given session, pair of phases to classify, and situation. An example input is: mi_sit_R_AO_train_list
#
# Results
# -------
# model_list: List of models. Each model is used to predict a phase for a particular individual where the first phase is 0 and the second phase is 1. For example, a model trained with mi_sit_R_AO_train_list will predict Resting as 0 and Action Observation as 1.
# """
fit_svm_model <- function(model_list, train_list){
for (i in seq(1, 8)){
model_list[[i]] <- tune(svm, y ~ ., data = train_list[[i]], kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 25, 50, 100, 1000)))
}
return(model_list)
}
# """Evaluate Results
#       Purpose
#       -------
#       This function will create predictions for each subject generated from a
#       linear svm model, create a confusion matrix of those predictions, report
#       the accuracy of the predictions, and ultimately calculate the mean and
#       standard error of the reported accuracies.
#
#       Parameters
#       ---------
#       model_list: A list of models used to form predictions. Models are
#                   anticipated to be tuned via svm and withhold a best.model.
#
#       test_list: A list of test observations used to make predictions and as a
#                  ground truth for comparison. Contains both test dependent and
#                  independent variables.
#
#      selected_subject: An integer used to select the subject for which
#                        predictions are desired. When NULL, all subjects will
#                        be evaluated for prediction.
#
#       Returns
#       -------
#       accuracy_collection: a collection used to store calculated accuracies.
#
#       Printed formatted output of the subject number, a confusion matrix of
#       prediction and truth values, the accuracy of each model per subject, and
#       the mean and standard error of the models accross all subjects.
# """
evaluate_results <- function(model_list, test_list, selected_subject=NULL){
if (is.null(selected_subject)){
prediction_list <- list()
accuracy_collection <- c(integer(length(model_list)))
}
for (i in seq(1, length(model_list))){
if(is.null(selected_subject)){
prediction_list[[i]] <- predict(model_list[[i]]$best.model, test_list[[i]])
f_print(sprintf("Subject %0.0f:", i))
cat("\n")
formatted_pred <- (as.integer(prediction_list[[i]]) - 1)
truth <- (as.integer(test_list[[i]]$y) - 1)
table <- table(truth = truth, pred = formatted_pred)
print(table)
accuracy_collection[[i]] <- (table[1] + table[4]) / sum(table) * 100
f_print(sprintf("Accuracy: %0.3f%%", accuracy_collection[[i]]))
cat("\n\n")
} else {
if(i == selected_subject){
prediction_list <- list()
accuracy_collection <- c(integer(1))
true_positive_rate_collection <- c(integer(1))
false_positive_rate_collection <- c(integer(1))
false_negative_rate_collection <- c(integer(1))
prediction_list[[i]] <- predict(model_list[[i]]$best.model, test_list[[i]])
f_print(sprintf("Subject %0.0f:",i))
cat("\n")
formatted_pred <- (as.integer(prediction_list[[i]]) - 1)
truth <- (as.integer(test_list[[i]]$y) - 1)
table <- table(truth = truth, pred = formatted_pred)
print(table)
accuracy_collection[[i]] <- (table[1] + table[4]) / sum(table) * 100
true_positive_rate_collection[[i]] <- (table[4]) / (table[4] + table[3]) * 100
false_positive_rate_collection[[i]] <- table[2] / (table[2] + table[1]) * 100
false_negative_rate_collection[[i]] <- table[3] / (table[3] + table[4]) * 100
f_print(sprintf("Accuracy: %0.3f%%", accuracy_collection[[i]]))
cat("\n")
f_print(sprintf("True Positive Rate: %0.3f%%", true_positive_rate_collection[[i]]))
cat("\n")
f_print(sprintf("False Positive Rate: %0.3f%%", false_positive_rate_collection[[i]]))
cat("\n")
f_print(sprintf("False Negative Rate: %0.3f%%", false_negative_rate_collection[[i]]))
cat("\n\n")
} else {
next
}
}
}
if (is.null(selected_subject)){
mean_accuracy <- mean(accuracy_collection)
f_print(sprintf("Mean Accuracy: %0.3f%%.", mean_accuracy))
cat("\n")
acc_standard_error <- sqrt(var(accuracy_collection))
f_print(sprintf("Standard Error: ±%0.3f%%.", acc_standard_error))
cat("\n")
}
return(accuracy_collection)
}
# Imports
import("sklearn")
import("mne")
import("numpy", as = "np")
import("random")
source_scripts()
reticulate::repl_python()
